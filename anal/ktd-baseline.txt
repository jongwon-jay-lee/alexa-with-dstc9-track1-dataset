root@pandora40:/work# python3 baseline.py --eval_only --checkpoint runs/ktd-baseline/ --eval_dataset val --dataroot data --no_labels --output_file pred/val/baseline.ktd.json
01/21/2021 03:15:04 - INFO - argument.py:15 : params_file is not set, using the params.json in checkpoint
01/21/2021 03:15:04 - INFO - configuration_utils.py:280 : loading configuration file runs/ktd-baseline/config.json
01/21/2021 03:15:04 - INFO - configuration_utils.py:318 : Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2ClsDoubleHeadsModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "decoder_start_token_id": null,
  "do_sample": false,
  "early_stopping": false,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "finetuning_task": null,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "is_decoder": false,
  "is_encoder_decoder": false,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "length_penalty": 1.0,
  "max_length": 20,
  "min_length": 0,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 12,
  "n_positions": 1024,
  "no_repeat_ngram_size": 0,
  "num_beams": 1,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": false,
  "pad_token_id": null,
  "prefix": null,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "use_bfloat16": false,
  "vocab_size": 50264
}

01/21/2021 03:15:04 - INFO - modeling_utils.py:505 : loading weights file runs/ktd-baseline/pytorch_model.bin
01/21/2021 03:15:08 - INFO - configuration_utils.py:280 : loading configuration file runs/ktd-baseline/config.json
01/21/2021 03:15:08 - INFO - configuration_utils.py:318 : Model config GPT2Config {
  "_num_labels": 1,
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2ClsDoubleHeadsModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "decoder_start_token_id": null,
  "do_sample": false,
  "early_stopping": false,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "finetuning_task": null,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "is_decoder": false,
  "is_encoder_decoder": false,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_epsilon": 1e-05,
  "length_penalty": 1.0,
  "max_length": 20,
  "min_length": 0,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 12,
  "n_positions": 1024,
  "no_repeat_ngram_size": 0,
  "num_beams": 1,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": false,
  "pad_token_id": null,
  "prefix": null,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "use_bfloat16": false,
  "vocab_size": 50264
}

01/21/2021 03:15:08 - INFO - tokenization_utils.py:420 : Model name 'runs/ktd-baseline/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'runs/ktd-baseline/' is a path, a model identifier, or url to a directory containing tokenizer files.
01/21/2021 03:15:08 - INFO - tokenization_utils.py:502 : loading file runs/ktd-baseline/vocab.json
01/21/2021 03:15:08 - INFO - tokenization_utils.py:502 : loading file runs/ktd-baseline/merges.txt
01/21/2021 03:15:08 - INFO - tokenization_utils.py:502 : loading file runs/ktd-baseline/added_tokens.json
01/21/2021 03:15:08 - INFO - tokenization_utils.py:502 : loading file runs/ktd-baseline/special_tokens_map.json
01/21/2021 03:15:08 - INFO - tokenization_utils.py:502 : loading file runs/ktd-baseline/tokenizer_config.json
01/21/2021 03:15:10 - INFO - main.py:373 : Training/evaluation parameters Namespace(adam_epsilon=1e-08, checkpoint='runs/ktd-baseline/', dataroot='data', dataset_args={'history_utterances': 1000000, 'history_max_tokens': 128, 'dataroot': 'data', 'knowledge_file': 'knowledge.json'}, device=device(type='cuda', index=0), distributed=False, eval_all_snippets=False, eval_dataset='val', eval_desc='', eval_only=True, exp_name='', fp16='', gradient_accumulation_steps=1, history_max_tokens=-1, knowledge_file='knowledge.json', knowledge_max_tokens=-1, labels_file=None, learning_rate=6.25e-05, local_rank=-1, max_grad_norm=1.0, model_name_or_path='./pretrained/gpt2/', n_gpu=4, negative_sample_method='', no_labels=True, num_train_epochs=10, output_dir='runs/ktd-baseline/', output_file='pred/val/baseline.ktd.json', params={'dataset_args': {'history_utterances': 1000000, 'history_max_tokens': 128, 'dataroot': 'data', 'knowledge_file': 'knowledge.json'}, 'task': 'detection', 'model_name_or_path': './pretrained/gpt2/', 'per_gpu_train_batch_size': 16, 'per_gpu_eval_batch_size': 16, 'gradient_accumulation_steps': 1, 'learning_rate': 6.25e-05, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 10, 'warmup_steps': 0, 'fp16': '', 'seed': 42}, params_file='runs/ktd-baseline/params.json', per_gpu_eval_batch_size=16, per_gpu_train_batch_size=16, seed=42, task='detection', warmup_steps=0)
01/21/2021 03:15:11 - INFO - dataset.py:57 : Tokenize and encode the dialog data
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9663/9663 [00:00<00:00, 686057.95it/s]
01/21/2021 03:15:11 - INFO - dataset.py:88 : Creating examples
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9663/9663 [00:15<00:00, 630.40it/s]
Evaluating:   0%|                                                                                                                                                                                           | 0/151 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 151/151 [00:58<00:00,  2.59it/s]
/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
01/21/2021 03:16:25 - INFO - data.py:56 : Writing predictions to pred/val/baseline.ktd.json
01/21/2021 03:16:25 - INFO - main.py:262 : ***** Eval results val *****
01/21/2021 03:16:25 - INFO - main.py:265 :   accuracy = 0.7295870847562869
01/21/2021 03:16:25 - INFO - main.py:265 :   loss = 4.6309517787781775
01/21/2021 03:16:25 - INFO - main.py:265 :   precision = 0.0
01/21/2021 03:16:25 - INFO - main.py:265 :   recall = 0.0
